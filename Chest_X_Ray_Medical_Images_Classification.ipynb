{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Steveglia/PneumoNet/blob/main/Chest_X_Ray_Medical_Images_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PneumoNet: Empowering Healthcare with AI-driven Pneumonia Detection #\n",
        "\n",
        "**<font size=\"3\">Early and accurate diagnosis in modern healthcare plays a pivotal role in improving patient outcomes. Our project, \"PneumoNet,\" harnesses the power of artificial intelligence to revolutionize the detection of pneumonia from chest X-rays. Pneumonia, a prevalent respiratory condition, requires prompt identification for timely intervention and optimal patient care.\n",
        "Our project utilizes a diverse dataset comprising chest X-ray images from reputable healthcare repositories. The dataset encompasses normal and pneumonia-affected cases, ensuring a robust model capable of discerning subtle radiological patterns indicative of the disease. Leveraging advanced deep learning techniques, we aim to develop an intelligent diagnostic tool capable of swiftly and accurately identifying pneumonia in chest X-ray images.\n",
        "The significance of our project lies in its potential to expedite diagnosis, reduce the workload on healthcare professionals, and enhance overall healthcare efficiency. With the integration of our AI-driven diagnostic solution, we aspire to facilitate timely treatment initiation, thereby improving patient outcomes and alleviating the strain on healthcare resources. PneumoNet represents a leap towards enhancing diagnostic capabilities and ultimately contributing to advancing healthcare practices.</font>**"
      ],
      "metadata": {
        "id": "IyJk05ReW1w0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing necessary libraries##\n"
      ],
      "metadata": {
        "id": "OKlzSa2zWns6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pathlib\n",
        "import imageio\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "1elOSj7JUAu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Oversampling - Blancing the dataset##\n",
        "\n",
        "**<font size=\"3\">Generating augmented images from the normal class to balance the training dataset. This ensures diversity in the augmented dataset by applying different data augmentation to each halves of the minority class. This helps prevent the model from overfitting to a specific set of augmented images and improves its ability to generalize to new, unseen data.**</font>"
      ],
      "metadata": {
        "id": "GnBaqjoGw3uV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate augmented images\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# Specify the directory name and its location\n",
        "desired_directory = \"/ChestXRay2017/chest_xray/train/NORMAL\"\n",
        "\n",
        "# Check if the directory already exists\n",
        "if not os.path.exists(desired_directory):\n",
        "    # If it doesn't exist, create the directory\n",
        "    os.makedirs(desired_directory)\n",
        "    print(f\"Directory '{desired_directory}' created successfully.\")\n",
        "else:\n",
        "    print(f\"Directory '{desired_directory}' already exists.\")\n",
        "\n",
        "# Example: Specify the directory containing your images\n",
        "image_directory = \"/ChestXRay2017/chest_xray/train/NORMAL/\"\n",
        "\n",
        "# Get a list of all image file names in the directory\n",
        "image_files = [f for f in os.listdir(image_directory) if f.endswith(('.jpeg'))]\n",
        "\n",
        "# Load the images using OpenCV\n",
        "images = [cv2.imread(os.path.join(image_directory, f)) for f in image_files]\n",
        "\n",
        "# Convert the images to NumPy arrays\n",
        "image_arrays = np.array(images)\n",
        "\n",
        "# Define your data augmentation parameters for the first half\n",
        "datagen_first_half = ImageDataGenerator(\n",
        "    rotation_range=8,\n",
        "    width_shift_range=0.05,\n",
        "    height_shift_range=0.05,\n",
        "    shear_range=0.15,\n",
        "    zoom_range=0.15,\n",
        "    fill_mode='nearest',\n",
        "    horizontal_flip=False\n",
        ")\n",
        "\n",
        "# Define your data augmentation parameters for the second half\n",
        "datagen_second_half = ImageDataGenerator(\n",
        "    rotation_range=13,  # Adjusted rotation\n",
        "    width_shift_range=0.1,  # Adjusted width shift\n",
        "    height_shift_range=0.1,  # Adjusted height shift\n",
        "    shear_range=0.25,  # Adjusted shear\n",
        "    zoom_range=0.25,  # Adjusted zoom\n",
        "    fill_mode='nearest',\n",
        "    horizontal_flip=False\n",
        ")\n",
        "\n",
        "# Specify the desired total number of augmented images\n",
        "desired_total_images = 2062\n",
        "batch_size = 16\n",
        "epochs = desired_total_images // batch_size\n",
        "first_half_count = 0\n",
        "second_half_count = 0\n",
        "\n",
        "# Assuming num_batches_per_epoch is calculated based on the original dataset size\n",
        "num_batches_per_epoch = len(image_arrays) // batch_size\n",
        "\n",
        "# Calculate the number of images to generate with slightly different data augmentation\n",
        "num_slightly_different = batch_size // 2\n",
        "\n",
        "# Counter to keep track of the current original image index\n",
        "generated_images_count = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for batch in range(num_batches_per_epoch):\n",
        "\n",
        "        # Generate and save augmented normal images\n",
        "        for i in range(batch_size):\n",
        "            # Break the loop if the desired number of augmented images is reached\n",
        "            if generated_images_count >= desired_total_images:\n",
        "                break\n",
        "\n",
        "            # Choose different data augmentation for at least half of the images\n",
        "            if i < num_slightly_different:\n",
        "                augmented_image = datagen_first_half.random_transform(image_arrays[i])\n",
        "                first_half_count += 1\n",
        "            else:\n",
        "                augmented_image = datagen_second_half.random_transform(image_arrays[i - num_slightly_different])\n",
        "                second_half_count += 1\n",
        "\n",
        "            # Update the count for each augmented image\n",
        "            generated_images_count += 1\n",
        "\n",
        "            # Save each augmented image with a unique filename\n",
        "            save_path = os.path.join(\"/ChestXRay2017/chest_xray/train/NORMAL\", f\"IM-{generated_images_count}.jpeg\")\n",
        "            plt.imsave(save_path, augmented_image)\n",
        "print(f\"{first_half_count} images created with the first data augmentation\")\n",
        "print(f\"{second_half_count} images created with the second data augmentation\")\n",
        "print(f\"{second_half_count + first_half_count} images created\")"
      ],
      "metadata": {
        "id": "HMRWMBymw3fP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Verify all normal images are unique##"
      ],
      "metadata": {
        "id": "mZBw7FRhz7A7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the number of unique images in a directory\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def count_unique_images(directory_path):\n",
        "    # Initialize a set to store unique image hashes\n",
        "    unique_image_hashes = set()\n",
        "\n",
        "    # Iterate through files in the directory\n",
        "    for filename in os.listdir(directory_path):\n",
        "        file_path = os.path.join(directory_path, filename)\n",
        "\n",
        "        # Check if the file is an image\n",
        "        if os.path.isfile(file_path) and any(file_path.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.gif']):\n",
        "            try:\n",
        "                # Open the image and calculate its hash\n",
        "                with Image.open(file_path) as img:\n",
        "                    image_hash = hash(img.tobytes())\n",
        "                    unique_image_hashes.add(image_hash)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "    # Return the count of unique images\n",
        "    return len(unique_image_hashes)\n",
        "\n",
        "# Replace 'your_directory_path' with the path to your image directory\n",
        "directory_path = '/ChestXRay2017/chest_xray/train/NORMAL'\n",
        "unique_image_count = count_unique_images(directory_path)\n",
        "\n",
        "print(f\"Number of unique images in {directory_path}: {unique_image_count}\")"
      ],
      "metadata": {
        "id": "NrQi2T5fz6c8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dataset File Paths and Summary Setup for Chest X-ray Analysis##\n",
        "\n",
        "**<font size=\"3\">Set up file paths for a chest X-ray dataset stored in different directories for training, testing, and validation. Then creates lists of file paths for images in categories like pneumonia and normal for each dataset split (train, test, and val). Finally, it prints the total count of images in each category to provide a quick summary of the dataset.**</font>"
      ],
      "metadata": {
        "id": "2ys_tgJcWHf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the base directory for the chest X-ray dataset\n",
        "base_dir = '/ChestXRay2017/chest_xray/'\n",
        "\n",
        "# Define directories for training data\n",
        "train_pneumonia_dir = base_dir + 'train/PNEUMONIA/'\n",
        "train_normal_dir = base_dir + 'train/NORMAL/'\n",
        "\n",
        "# Define directories for testing data\n",
        "test_pneumonia_dir = base_dir + 'test/PNEUMONIA/'\n",
        "test_normal_dir = base_dir + 'test/NORMAL/'\n",
        "\n",
        "# Define directories for validation data\n",
        "val_normal_dir = base_dir + 'val/NORMAL/'\n",
        "val_pneumonia_dir = base_dir + 'val/PNEUMONIA/'\n",
        "\n",
        "# Create lists of file paths for images in each category\n",
        "train_pn = [train_pneumonia_dir + \"{}\".format(i) for i in os.listdir(train_pneumonia_dir)]\n",
        "train_normal = [train_normal_dir + \"{}\".format(i) for i in os.listdir(train_normal_dir)]\n",
        "\n",
        "test_normal = [test_normal_dir + \"{}\".format(i) for i in os.listdir(test_normal_dir)]\n",
        "test_pn = [test_pneumonia_dir + \"{}\".format(i) for i in os.listdir(test_pneumonia_dir)]\n",
        "\n",
        "val_pn = [val_pneumonia_dir + \"{}\".format(i) for i in os.listdir(val_pneumonia_dir)]\n",
        "val_normal = [val_normal_dir + \"{}\".format(i) for i in os.listdir(val_normal_dir)]\n",
        "\n",
        "# Print total counts of images in different categories\n",
        "print(\"Total images:\", len(train_pn + train_normal + test_normal + test_pn + val_pn + val_normal))\n",
        "print(\"Total pneumonia images:\", len(train_pn + test_pn + val_pn))\n",
        "print(\"Total Normal images:\", len(train_normal + test_normal + val_normal))"
      ],
      "metadata": {
        "id": "V1HEVR1WUvLS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b18addd8-a106-4370-d6fd-ba7149aa9a58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images: 5873\n",
            "Total pneumonia images: 4282\n",
            "Total Normal images: 1591\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Preprocessing & Visualization - Original Dataset##\n",
        "**<font size=\"3\">The dataset is relatively small, consisting of 5873 chest X-ray images with pneumonia and normal conditions. To maximize the utility of the limited data, we adopt an 80-15-5 split strategy, dividing the dataset into training, testing, and validation sets. This distribution ensures that a substantial portion (80%) is allocated to training, allowing the model to learn patterns effectively. The 15% reserved for testing is an independent benchmark to evaluate the model's generalization performance. The remaining 5% set aside for validation aids in fine-tuning the model and preventing overfitting. This partitioning strategy balances training with available data and rigorously assesses the model's performance on unseen instances, considering the constraints posed by the dataset's size. </font>**"
      ],
      "metadata": {
        "id": "zBZr9FanZ4Nm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Splitting (train 80%, test 15%, and validation 5%)\n",
        "\n",
        "# Combining pneumonia and normal chest X-ray images into two Python lists\n",
        "pn = train_pn + test_pn + val_pn\n",
        "normal = train_normal + test_normal + val_normal\n",
        "\n",
        "# Splitting the dataset into train set, test set, and validation set\n",
        "train_imgs = pn[:3418] + normal[:1224]  # 80% of 4642 Pneumonia and normal chest X-ray are 3418 and 1224, respectively.\n",
        "test_imgs = pn[3418:4059] + normal[1224:1502] # 15% of 919 Pneumonia and normal chest X-ray are 641 and 278, respectively.\n",
        "val_imgs = pn[4059:] + normal[1502:] # 5% of 312 Pneumonia and normal chest X-ray are 223 and 89, respectively.\n",
        "\n",
        "# Displaying the distribution of images in each set\n",
        "print(\"Total Train Images: %s containing %s pneumonia and %s normal images\"\n",
        "      % (len(train_imgs), len(pn[:3418]), len(normal[:1224])))\n",
        "print(\"Total Test Images: %s containing %s pneumonia and %s normal images\"\n",
        "      % (len(test_imgs), len(pn[3418:4059]), len(normal[1224:1502])))\n",
        "print(\"Total Validation Images: %s containing %s pneumonia and %s normal images\"\n",
        "      % (len(val_imgs), len(pn[4059:]), len(normal[1502:])))\n",
        "\n",
        "# Randomly shuffling the images in each set\n",
        "import random\n",
        "random.shuffle(train_imgs)\n",
        "random.shuffle(test_imgs)\n",
        "random.shuffle(val_imgs)"
      ],
      "metadata": {
        "id": "VCEdt8duZ5Zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Preprocessing & Visualization - Augmented Dataset##\n",
        "**<font size=\"3\">The dataset is relatively small, consisting of 8529 chest X-ray images with pneumonia and normal conditions. To maximize the utility of the limited data, we adopt an 86-13-1 split strategy, dividing the dataset into training, testing, and validation sets. This distribution ensures that a substantial portion (86%) is allocated to training, allowing the model to learn patterns effectively. The 13% reserved for testing is an independent benchmark to evaluate the model's generalization performance. The remaining 1% set aside for validation aids in fine-tuning the model and preventing overfitting. This partitioning strategy balances training with available data and rigorously assesses the model's performance on unseen instances, considering the constraints posed by the dataset's size. </font>**"
      ],
      "metadata": {
        "id": "iTyHFGUH2NFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Splitting (train 86%, test 13%, and validation 1%)\n",
        "\n",
        "# Combining pneumonia and normal chest X-ray images into two Python lists\n",
        "pn = train_pn + test_pn + val_pn\n",
        "normal = train_normal + test_normal + val_normal\n",
        "\n",
        "# Splitting the dataset into train set, test set, and validation set\n",
        "train_imgs = pn[:3411] + normal[:3411]  # 86% of 6822 Pneumonia and normal chest X-ray are 3411 and 3411, respectively.\n",
        "test_imgs = pn[3411:4222] + normal[3411:3615] # 13% of 919 Pneumonia and normal chest X-ray are 811 and 204, respectively.\n",
        "val_imgs = pn[4222:] + normal[3615:] # 1% of 312 Pneumonia and normal chest X-ray are 60 and 38, respectively.\n",
        "\n",
        "# Displaying the distribution of images in each set\n",
        "print(\"Total Train Images: %s containing %s pneumonia and %s normal images\"\n",
        "      % (len(train_imgs), len(pn[:3411]), len(normal[:3411])))\n",
        "print(\"Total Test Images: %s containing %s pneumonia and %s normal images\"\n",
        "      % (len(test_imgs), len(pn[3411:4222]), len(normal[3411:3615])))\n",
        "print(\"Total Validation Images: %s containing %s pneumonia and %s normal images\"\n",
        "      % (len(val_imgs), len(pn[4222:]), len(normal[3615:])))\n",
        "\n",
        "# Randomly shuffling the images in each set\n",
        "import random\n",
        "random.shuffle(train_imgs)\n",
        "random.shuffle(test_imgs)\n",
        "random.shuffle(val_imgs)"
      ],
      "metadata": {
        "id": "XiCu2_Bs2L4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess the images ##\n",
        "**<font size=\"3\">The purpose of this code is to prepare a dataset for training a machine learning model on chest X-ray images, ensuring uniformity in size, color channels, and labeling for further analysis and model development. The preprocessing steps help create a consistent and standardized input for a machine learning algorithm. </font>**"
      ],
      "metadata": {
        "id": "AQ7V0pqugJVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "img_size = 224\n",
        "\n",
        "def preprocess_image(image_list):\n",
        "    \"\"\"\n",
        "    Preprocesses a list of image file paths.\n",
        "\n",
        "    Args:\n",
        "    - image_list (list): List of file paths for images.\n",
        "\n",
        "    Returns:\n",
        "    - X (list): Processed images.\n",
        "    - y (list): Labels (0 for Normal or 1 for Pneumonia).\n",
        "    \"\"\"\n",
        "\n",
        "    X = [] # Images\n",
        "    y = [] # Labels (0 for Normal or 1 for Pneumonia)\n",
        "    count = 0\n",
        "\n",
        "    for image in image_list:\n",
        "\n",
        "        try:\n",
        "\n",
        "            # Read and convert the image\n",
        "            img = cv2.imread(image, cv2.IMREAD_GRAYSCALE) # Read the image in grayscale\n",
        "            img = cv2.resize(img, (img_size, img_size), interpolation=cv2.INTER_CUBIC) # Resize the image to the target size\n",
        "            img = np.dstack([img, img, img]) # Convert the grayscale image to a 3D RGB image\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Convert the image from BGR to RGB color space. This is done to ensure the image has three channels (RGB).\n",
        "\n",
        "            # Normalalize Image\n",
        "            img = img.astype(np.float32)/255.\n",
        "\n",
        "            count += 1\n",
        "            X.append(img)\n",
        "\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        # Get the labels\n",
        "        if 'NORMAL' in image or 'IM' in image:\n",
        "            y.append(0)\n",
        "        elif 'virus' in image or 'bacteria' in image:\n",
        "            y.append(1)\n",
        "\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "33VaD6UmgJlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocess train images##"
      ],
      "metadata": {
        "id": "0tKzqz1M0V2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the labels for train set\n",
        "X, y = preprocess_image(train_imgs)"
      ],
      "metadata": {
        "id": "aLTh2RIk0WMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Check if all train images getting labels##"
      ],
      "metadata": {
        "id": "Md4Q_QQB1qn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arr = y\n",
        "# Get a tuple of unique values & their frequency in numpy array\n",
        "uniqueValues, occurCount = np.unique(arr, return_counts=True)\n",
        "\n",
        "# Informative printing labels alongside their occurrence counts.\n",
        "for label, count in zip(uniqueValues, occurCount):\n",
        "    print(f\"Label {label}: Occurs {count} times\")"
      ],
      "metadata": {
        "id": "fGX4uciP1q2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Display some images from train set##"
      ],
      "metadata": {
        "id": "N1xYqPi7LxMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(20, 5))\n",
        "k = 1\n",
        "for i in range(4):\n",
        "    a = fig.add_subplot(1, 4, k)\n",
        "    if (y[i] == 0):\n",
        "        a.set_title('Normal')\n",
        "    else:\n",
        "        a.set_title('Pneumonia')\n",
        "\n",
        "    plt.imshow(X[i])\n",
        "    k += 1;"
      ],
      "metadata": {
        "id": "pABSOopPLxny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocess test images##"
      ],
      "metadata": {
        "id": "wNl58IvcL18A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the labels for test set\n",
        "P, t = preprocess_image(test_imgs)"
      ],
      "metadata": {
        "id": "iP40lvPRL2dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Check if all test images getting labels##"
      ],
      "metadata": {
        "id": "ZUHSj3dbL4UF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arr = t\n",
        "# Get a tuple of unique values & their frequency in numpy array\n",
        "uniqueValues, occurCount = np.unique(arr, return_counts=True)\n",
        "\n",
        "# Informative printing labels alongside their occurrence counts.\n",
        "for label, count in zip(uniqueValues, occurCount):\n",
        "    print(f\"Label {label}: Occurs {count} times\")"
      ],
      "metadata": {
        "id": "8vxVQrZFL3DG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Display some images from test set##"
      ],
      "metadata": {
        "id": "8Y9PQBNHL7Fu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(20, 5))\n",
        "k = 1\n",
        "for i in range(4):\n",
        "    a = fig.add_subplot(1, 4, k)\n",
        "    if (t[i] == 0):\n",
        "        a.set_title('Normal')\n",
        "    else:\n",
        "        a.set_title('Pneumonia')\n",
        "\n",
        "    plt.imshow(P[i])\n",
        "    k += 1;"
      ],
      "metadata": {
        "id": "9aCFb-voL7cG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocess validation images##"
      ],
      "metadata": {
        "id": "AT4-cFfTL9g0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the labels for validation set\n",
        "K, m = preprocess_image(val_imgs)"
      ],
      "metadata": {
        "id": "fg6ardvZL95h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Check if all validation images getting labels##"
      ],
      "metadata": {
        "id": "Yhl0rqO4L_44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arr = m\n",
        "# Get a tuple of unique values & their frequency in numpy array\n",
        "uniqueValues, occurCount = np.unique(arr, return_counts=True)\n",
        "\n",
        "# Informative printing labels alongside their occurrence counts.\n",
        "for label, count in zip(uniqueValues, occurCount):\n",
        "    print(f\"Label {label}: Occurs {count} times\")"
      ],
      "metadata": {
        "id": "Odo6-cIvMBL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Display some images from validation set##"
      ],
      "metadata": {
        "id": "TTMORfKFMDUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(20, 5))\n",
        "k = 1\n",
        "for i in range(4):\n",
        "    a = fig.add_subplot(1, 4, k)\n",
        "    if (m[i] == 0):\n",
        "        a.set_title('Normal')\n",
        "    else:\n",
        "        a.set_title('Pneumonia')\n",
        "\n",
        "    plt.imshow(K[i])\n",
        "    k += 1;"
      ],
      "metadata": {
        "id": "o9Bb4J5tMDm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Understanding the balance or imbalance of data in different datasets##\n",
        "**<font size=\"3\">Creates a DataFrame from three arrays and visualizes the distribution of data across the 'Train', 'Test', and 'Val' datasets using countplots.</font>**"
      ],
      "metadata": {
        "id": "dRkJk4l3upLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame()\n",
        "df['Train'] = y\n",
        "df['Test'] = pd.Series(t)\n",
        "df['Val'] = pd.Series(m)\n",
        "\n",
        "data_size = 3500\n",
        "\n",
        "# Create subplots\n",
        "fig, ax = plt.subplots(1, 3, figsize=(15, 7))\n",
        "\n",
        "# Adjust countplot parameters and set y-axis limits\n",
        "sns.countplot(x='Train', data=df, ax=ax[0])\n",
        "ax[0].set_ylim(0, data_size)\n",
        "\n",
        "sns.countplot(x='Test', data=df, ax=ax[1])\n",
        "ax[1].set_ylim(0, data_size)\n",
        "\n",
        "sns.countplot(x='Val', data=df, ax=ax[2])\n",
        "ax[2].set_ylim(0, data_size)\n",
        "\n",
        "# Display the figure\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cAl_slpTup5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dealing with class imbalance with class weights - Use only for the original dataset##\n",
        "\n",
        "**<font size=\"3\">In imbalanced datasets, where certain classes have significantly fewer samples than others, models might be biased towards the majority class during training. Assigning class weights helps the model give more importance to the minority class, ensuring that the model is not dominated by the majority class. By using class weights during training, the model is encouraged to pay more attention to the underrepresented classes, leading to a more balanced and fair learning process.</font>**"
      ],
      "metadata": {
        "id": "q6zn2ejJO2br"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "class_weights = class_weight.compute_class_weight(class_weight='balanced',\n",
        "                                                 classes=np.unique(y), # here, y contains train set label\n",
        "                                                 y=y)\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "print(class_weights)"
      ],
      "metadata": {
        "id": "Hk4xG35BO2zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepares and processes image data ##\n",
        "\n",
        "**<font size=\"3\">Prepare and processes image data for a machine learning model by combining two sets of images and converting the images and labels into NumPy arrays.</font>**"
      ],
      "metadata": {
        "id": "QxFiAIGVATQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the processed images and labels into NumPy arrays\n",
        "X_train = np.array(X)\n",
        "y_train = np.array(y)\n",
        "X_test = np.array(P)\n",
        "y_test = np.array(t)\n",
        "X_val = np.array(K)\n",
        "y_val = np.array(m)\n",
        "\n",
        "# Print the shapes of the arrays\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)"
      ],
      "metadata": {
        "id": "iI3SmgOeATn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training##\n",
        "\n",
        "**<font size=\"3\">We will use a batch size of 16. Batch size should be a power of 2. The batch size 16 means the model will train 16 training samples and then update its parameters once. Batch training is faster and memory efficient.</font>**"
      ],
      "metadata": {
        "id": "-HzCcs71AdEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the length of the train, test and validation data\n",
        "ntrain = len(X_train)\n",
        "nval = len(X_val)\n",
        "ntest = len(X_test)\n",
        "\n",
        "# Setting batch size\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "NZsXRTGXAdlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation ##\n",
        "**<font size=\"3\">We employ data augmentation to artificially expand our dataset, especially given its relatively small size. Mild data augmentation is applied exclusively to the training data, mirroring the real-world scenario of standardized chest X-ray images, which the machine will encounter during testing. This approach not only aids in preventing overfitting but also aligns the training process with the expected characteristics of the test data.</font>**"
      ],
      "metadata": {
        "id": "XQ1UHGVXAtD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Data augmentation for the training set\n",
        "train_datagen = ImageDataGenerator(  rotation_range=7,\n",
        "                                     width_shift_range=0.05,\n",
        "                                     height_shift_range=0.05,\n",
        "                                     #shear_range=0.2,\n",
        "                                     zoom_range=0.2,\n",
        "                                     horizontal_flip=True)\n",
        "\n",
        "# Data augmentation for the test set\n",
        "test_datagen = ImageDataGenerator()\n",
        "\n",
        "# Data augmentation for the validation set\n",
        "val_datagen = ImageDataGenerator()"
      ],
      "metadata": {
        "id": "It2x0c8qAtkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the image generators\n",
        "train_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size)\n",
        "val_generator = val_datagen.flow(X_val, y_val, batch_size=batch_size)\n",
        "test_generator = test_datagen.flow(X_test, y_test, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "_LbbPTf3Ayot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set image Size\n",
        "img_size = 224"
      ],
      "metadata": {
        "id": "ZNETb32MA67u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train full train set with CNN Giovanna  ##\n",
        "\n",
        "**<font size=\"3\"> We build a convolutional neural network (CNN) architecture from scratch. </font>**"
      ],
      "metadata": {
        "id": "BdPIJqEIA9v9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "callbacks1 = [\n",
        "    EarlyStopping(monitor = 'loss', patience = 6),\n",
        "    ReduceLROnPlateau(monitor = 'loss', patience = 3, verbose=1),\n",
        "    ]"
      ],
      "metadata": {
        "id": "oUPzErXb4pQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import BatchNormalization, Activation\n",
        "\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, BatchNormalization, Flatten, Dense, Activation\n",
        "def Giovanna():\n",
        "  layers = [\n",
        "        Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(224, 224, 3)),\n",
        "        Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "        Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "        Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "        Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "        Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        Flatten(),\n",
        "        Dense(1024, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "\n",
        "        Dense(512, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.4),\n",
        "\n",
        "        Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        Dense(1, activation='sigmoid')\n",
        "\n",
        "      ]\n",
        "\n",
        "\n",
        "\n",
        "  model = Sequential(layers)\n",
        "  return model\n",
        "\n"
      ],
      "metadata": {
        "id": "A45uKWBRA_TO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "outputId": "c8b31421-2408-4530-e029-a07cd0d055d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c0909b9d0a8b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregularizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__internal__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/__internal__/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/__internal__/backend/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_initialize_variables\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minitialize_variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrack_variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \"\"\"\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFunctional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# from tensorflow.python import keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# from tensorflow.python.layers import layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msaved_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtpu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/saved_model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfingerprinting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFingerprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfingerprinting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_fingerprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# pylint: enable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgraph_view\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrestore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvalues_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mag_ctx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mautograph_ctx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollective_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/experimental/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_to_cardinality\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_to_cardinality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsing_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_example_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetching_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcopy_to_device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetching_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprefetch_to_device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_access\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/experimental/ops/prefetching_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_experimental_dataset_ops\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mged_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/functional_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mwarn_once\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     back_prop=False)\n\u001b[0;32m--> 172\u001b[0;31m def foldl_v2(fn,\n\u001b[0m\u001b[1;32m    173\u001b[0m              \u001b[0melems\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m              \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36madd_dispatch_support\u001b[0;34m(target, iterable_parameters)\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1281\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mdecorator\u001b[0;34m(dispatch_target)\u001b[0m\n\u001b[1;32m   1271\u001b[0m     op_dispatch_handler = tf_decorator.make_decorator(dispatch_target,\n\u001b[1;32m   1272\u001b[0m                                                       op_dispatch_handler)\n\u001b[0;32m-> 1273\u001b[0;31m     \u001b[0madd_type_based_api_dispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1274\u001b[0m     api_dispatcher = getattr(op_dispatch_handler, TYPE_BASED_DISPATCH_ATTR,\n\u001b[1;32m   1275\u001b[0m                              None)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36madd_type_based_api_dispatcher\u001b[0;34m(target)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m   \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m   \u001b[0mtarget_argspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_inspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munwrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtarget_argspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvarargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtarget_argspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;31m# @TODO(b/194903203) Add v2 dispatch support for APIs that take varargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/tf_inspect.py\u001b[0m in \u001b[0;36mgetargspec\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    158\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;31m# Python3 will handle most callables here (not partial).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_getargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/tf_inspect.py\u001b[0m in \u001b[0;36m_getargspec\u001b[0;34m(target)\u001b[0m\n\u001b[1;32m     90\u001b[0m       \u001b[0;32mfrom\u001b[0m \u001b[0mFullArgSpec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \"\"\"\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mfullargspecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mdefaults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfullargspecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/tf_inspect.py\u001b[0m in \u001b[0;36mgetfullargspec\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator_argspec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_convert_maybe_argspec_to_fullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator_argspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_getfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36mgetfullargspec\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m   1296\u001b[0m     \u001b[0mkwdefaults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1298\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_annotation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1299\u001b[0m         \u001b[0mannotations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'return'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_annotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36mreturn_annotation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3008\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3010\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3011\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreturn_annotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3012\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_return_annotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(learning_rate=0.001)  # Reducing learning rate\n",
        "model = Giovanna()\n",
        "model.compile(optimizer = optimizer, loss = 'binary_crossentropy' , metrics = ['accuracy','mae'])\n"
      ],
      "metadata": {
        "id": "stg033S6BDUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We train for 32 epochs\n",
        "history = model.fit(train_generator,\n",
        "                              steps_per_epoch= ntrain // 32,\n",
        "                              epochs=60,\n",
        "                              callbacks = callbacks1,\n",
        "                              validation_data=val_generator,\n",
        "                              validation_steps=nval // batch_size,\n",
        "                              class_weight =class_weights,\n",
        ")"
      ],
      "metadata": {
        "id": "uSXesTv7BPVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the model on the testing data  ##"
      ],
      "metadata": {
        "id": "-8GI3yUxBTvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test data\n",
        "test_results = model.evaluate(test_generator, steps=ntest // batch_size, verbose=1)\n",
        "print(\"Test Loss:\", test_results[0])\n",
        "print(\"Test Accuracy:\", test_results[1])"
      ],
      "metadata": {
        "id": "97qL7-k5BUcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the training and validation accuracy and loss ##\n"
      ],
      "metadata": {
        "id": "DK9IX7RCBZIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets plot the train and val curve\n",
        "# Get the details form the history object\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "#Train and validation accuracy\n",
        "plt.plot(epochs, acc, 'b', label='Training accurarcy')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation accurarcy')\n",
        "plt.title('Training and Validation accurarcy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "#Train and validation loss\n",
        "plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N3Qf38gQBZnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estimation of classification performance / Result ##\n",
        "**<font size=\"3\">We will assess the classification performance of our model using various evaluation metrics. Employing multiple metrics is crucial to thoroughly evaluate the model's correctness and optimization. Our examination will include metrics such as Accuracy, Recall, Precision, F1 score, and AUC score to comprehensively gauge the model's performance. </font>**"
      ],
      "metadata": {
        "id": "KT9TecegBdHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "preds = model.predict(X_test)\n",
        "\n",
        "acc = accuracy_score(y_test, np.round(preds)) * 100\n",
        "cm = confusion_matrix(y_test, np.round(preds))\n",
        "\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "print('CONFUSION MATRIX ------------------')\n",
        "print(cm)\n",
        "\n",
        "print('\\n============TEST METRICS=============')\n",
        "precision = tp/(tp+fp) * 100\n",
        "recall = tp/(tp+fn) * 100\n",
        "print('Accuracy: {}%'.format(acc))\n",
        "print('Precision: {}%'.format(precision))\n",
        "print('Recall: {}%'.format(recall))\n",
        "print('F1-score: {}'.format(2*precision*recall/(precision+recall)))\n",
        "\n",
        "print('\\nTRAIN METRIC ----------------------')\n",
        "print('Train acc: {}'.format(np.round((history.history['accuracy'][-1]) * 100, 2)))"
      ],
      "metadata": {
        "id": "P0TNj7KOBdwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\",)"
      ],
      "metadata": {
        "id": "U66ldLEOBi9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the ROC ##\n",
        "**<font size=\"3\">The ROC (receiver operating characteristic) curve indicates the diagnostic accuracy and porformance of a model.We show the ROC curve and also calculate AUC score</font>**"
      ],
      "metadata": {
        "id": "yfiFCN_OBmOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "from sklearn.metrics import auc\n",
        "\n",
        "fpr , tpr , thresholds = roc_curve ( y_test , preds)\n",
        "auc_keras = auc(fpr, tpr)\n",
        "print(\"AUC Score:\",auc_keras)\n",
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange',\n",
        "         lw=lw, label='ROC curve (area = %0.2f)' % auc_keras)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic example')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bTHccNUnBnQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross-Validation##\n",
        "\n",
        "**<font size=\"3\">Cross-validation is a valuable technique for obtaining a robust evaluation of a model's performance, especially when dealing with limited training data. It provides insights into how well the model generalizes to different subsets, helping to make more informed decisions about its suitability for unseen data.</font>**"
      ],
      "metadata": {
        "id": "JgpSyf7HAK-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.set(style= \"darkgrid\", color_codes = True)\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, Input, GlobalAveragePooling2D\n",
        "from keras.regularizers import l2\n",
        "from keras.metrics import Precision, Recall, BinaryAccuracy\n",
        "from sklearn.metrics import roc_curve, auc, accuracy_score, confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from keras.preprocessing.image import img_to_array, load_img\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "# Initialize StratifiedKFold\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "datagen = ImageDataGenerator(rotation_range=7,\n",
        "                             width_shift_range=0.05,\n",
        "                             height_shift_range=0.05,\n",
        "                             #shear_range=0.2,\n",
        "                             zoom_range=0.2,\n",
        "                             horizontal_flip=True\n",
        ")\n",
        "\n",
        "\n",
        "# Initialize an empty list to store accuracy scores\n",
        "accuracy_scores = []\n",
        "c = 1\n",
        "batch = 32\n",
        "# Iterate through each fold\n",
        "for train_index, test_index in kf.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    train_generator = datagen.flow(X_train, y_train, batch_size = batch)\n",
        "\n",
        "     # Initialize and compile the model\n",
        "    model = Giovanna()\n",
        "    model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    print(f\"starting subsequence {c}:\")\n",
        "    # Train the model\n",
        "    model.fit(      train_generator,\n",
        "                    steps_per_epoch=len(X_train) // batch,\n",
        "                    epochs=60,\n",
        "                    callbacks = callbacks1,\n",
        "    )\n",
        "    print(f\"finish subsequence {c}\")\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "\n",
        "    evaluation_results1 = model.evaluate(X_test, y_test, verbose = 0)\n",
        "    print(f\"Test loss: {evaluation_results1[0] * 100:.2f}%, Test accuracy: {evaluation_results1[1] * 100:.2f}%\")\n",
        "\n",
        "    y_pred_prob = model.predict(X_test)\n",
        "\n",
        "    # Convert probabilities to binary predictions using a threshold (e.g., 0.5)\n",
        "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_scores.append(accuracy)\n",
        "    print(f\"\\nThe accuracy for the subsequence {c} is: {accuracy}\")\n",
        "    model.save(f'mk{c}.h5')\n",
        "    c += 1\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "# Print the cross-validated accuracy\n",
        "\n",
        "print(\"Cross-validated Accuracy: {:.2f}%\".format(np.mean(accuracy_scores) * 100))\n",
        "print(f\"\\n this is the accuracy for:\\nSubsequence 1 {accuracy_score[0]}\\nSubsequence 2 {accuracy_score[1]}\\nSubsequence 3 {accuracy_score[2]}\\nSubsequence 4 {accuracy_score[3]}\\nSubsequence 5 {accuracy_score[4]} \")\n"
      ],
      "metadata": {
        "id": "Lg11h5O-AQdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi model combination ##\n",
        "\n",
        "**<font size=\"3\">After developing four distinct models, we merge their outcomes to enhance accuracy.\n",
        "The algorithm assesses predictions from each model, returning the most probable result.\n",
        "However, when all four models yield identical predictions for opposing outcomes, we prioritize the best prediction based on the models' class accuracy.</font>**"
      ],
      "metadata": {
        "id": "aHhjdItTD_vD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing all the four different models and store them in variables. The path in the following code need to changed to your actual correct path.\n",
        "from keras.models import load_model\n",
        "mk54 = load_model('/content/drive/MyDrive/AI/chest_xray/Giovanna_oversampling_callbacks.h5')\n",
        "bagley = load_model('/content/drive/MyDrive/AI/MMC/bagle 6.h5')\n",
        "lola = load_model('/content/drive/MyDrive/AI/MMC/lola.h5')\n",
        "giovanna = load_model('/content/drive/MyDrive/AI/chest_xray/Giovanna_weight_callbacks.h5')"
      ],
      "metadata": {
        "id": "Xm1UFDnF7EU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculating the predicyion of each model\n",
        "predictions_MK54 = mk54.predict(X_test)\n",
        "predictions_Bagley = bagley.predict(X_test)\n",
        "predictions_Giovanna = giovanna.predict(X_test)\n",
        "predictions_Lola = lola.predict(X_test)"
      ],
      "metadata": {
        "id": "ED1JfmeR7Qxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming y_val contains the true labels for the validation set\n",
        "d_fra = {}\n",
        "\n",
        "\n",
        "final_predictions = []\n",
        "final_predictions_fra = []\n",
        "c = 0\n",
        "for i in range(len(predictions_MK54)):\n",
        "  final_predictions.append((predictions_Bagley[i][0] + predictions_MK54[i][0] + predictions_Giovanna[i][0] + predictions_Lola[i][0])/ 4)\n",
        "\n",
        "\n",
        "for i in range(len(predictions_MK54)):\n",
        "  l = [0] * 4\n",
        "  l[0] = predictions_Bagley[i][0]\n",
        "  l[1] = predictions_MK54[i][0]\n",
        "  l[2] = predictions_Giovanna[i][0]\n",
        "  l[3] = predictions_Lola[i][0]\n",
        "\n",
        "  d_fra[i] = l\n",
        "  c = 0\n",
        "  for v in range(len(l)):\n",
        "    if l[v] > 0.5:\n",
        "      c += 1\n",
        "    else:\n",
        "      c -= 1\n",
        "  if c > 0:\n",
        "    final_predictions_fra.append(max(l))\n",
        "  elif c == 0:\n",
        "    if l[0] > 0.5 or l[1] > 0.5 and l[2] < 0.5 or l[3] < 0.5:\n",
        "      final_predictions_fra.append(max(l))\n",
        "    elif l[2] > 0.5 or l[3] > 0.5 and l[0] < 0.5 or l[1] < 0.5:\n",
        "      final_predictions_fra.append(min(l))\n",
        "  else:\n",
        "    final_predictions_fra.append(min(l))\n",
        "\n",
        "\n",
        "\n",
        "final_predictions_array = np.array(final_predictions)\n",
        "final_predictions_combined_models_array = np.array(final_predictions_fra)\n",
        "\n",
        "# print(final_predictions)\n",
        "\n",
        "\n",
        "# Set a threshold for binary classification\n",
        "threshold = 0.5\n",
        "\n",
        "# Round the predicted probabilities to binary predictions\n",
        "predictions_binary_Bagley = (predictions_Bagley > threshold).astype(int)\n",
        "predictions_binary_MK54 = (predictions_MK54 > threshold).astype(int)\n",
        "predictions_binary_Giovanna = (predictions_Giovanna > threshold).astype(int)\n",
        "predictions_binary_Lola = (predictions_Lola > threshold).astype(int)\n",
        "predictions_binary_mean = (final_predictions_array > threshold).astype(int)\n",
        "predictions_binary_combined_models = (final_predictions_combined_models_array > threshold).astype(int)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create a DataFrame for better visualization (optional, requires pandas)\n",
        "import pandas as pd\n",
        "\n",
        "df_Bagley = pd.DataFrame({\n",
        "    'True Label': y_test.flatten(),  # Flatten if y_val is not a 1D array\n",
        "    'Predicted Binary': predictions_binary_Bagley.flatten(),\n",
        "    'Predicted Probability': predictions_Bagley.flatten()\n",
        "})\n",
        "\n",
        "df_MK54 = pd.DataFrame({\n",
        "    'True Label': y_test.flatten(),  # Flatten if y_val is not a 1D array\n",
        "    'Predicted Binary': predictions_binary_MK54.flatten(),\n",
        "    'Predicted Probability': predictions_MK54.flatten()\n",
        "})\n",
        "\n",
        "df_Giovanna = pd.DataFrame({\n",
        "    'True Label': y_test.flatten(),  # Flatten if y_val is not a 1D array\n",
        "    'Predicted Binary': predictions_binary_Giovanna.flatten(),\n",
        "    'Predicted Probability': predictions_Giovanna.flatten()\n",
        "})\n",
        "\n",
        "df_Lola = pd.DataFrame({\n",
        "    'True Label': y_test.flatten(),  # Flatten if y_val is not a 1D array\n",
        "    'Predicted Binary': predictions_binary_Lola.flatten(),\n",
        "    'Predicted Probability': predictions_Lola.flatten()\n",
        "})\n",
        "\n",
        "df_mean = pd.DataFrame({\n",
        "    'True Label': y_test.flatten(),  # Flatten if y_val is not a 1D array\n",
        "    'Predicted Binary': predictions_binary_mean.flatten(),\n",
        "    'Predicted Probability': final_predictions_array.flatten()\n",
        "})\n",
        "\n",
        "\n",
        "df_combined_models = pd.DataFrame({\n",
        "    'True Label': y_test.flatten(),  # Flatten if y_val is not a 1D array\n",
        "    'Predicted Binary': predictions_binary_combined_models.flatten(),\n",
        "    'Predicted Probability': final_predictions_combined_models_array.flatten()\n",
        "})\n",
        "# Display the DataFrame\n",
        "# print(df)\n",
        "df_Bagley['Predicted Probability (%)'] = (df_Bagley['Predicted Probability']).round(2)\n",
        "df_MK54['Predicted Probability (%)'] = (df_MK54['Predicted Probability']).round(2)\n",
        "df_Giovanna['Predicted Probability (%)'] = (df_Giovanna['Predicted Probability']).round(2)\n",
        "df_Lola['Predicted Probability (%)'] = (df_Lola['Predicted Probability']).round(2)\n",
        "\n",
        "df_mean['Predicted Probability (%)'] = (df_mean['Predicted Probability']).round(2)\n",
        "df_combined_models['Predicted Probability (%)'] = (df_combined_models['Predicted Probability']).round(2)\n",
        "\n",
        "df_Bagley = df_Bagley.drop(columns=['Predicted Probability'])\n",
        "df_MK54 = df_MK54.drop(columns=['Predicted Probability'])\n",
        "df_Giovanna = df_Giovanna.drop(columns=['Predicted Probability'])\n",
        "df_Lola = df_Lola.drop(columns=['Predicted Probability'])\n",
        "\n",
        "df_mean = df_mean.drop(columns=['Predicted Probability'])\n",
        "df_combined_models = df_combined_models.drop(columns=['Predicted Probability'])\n",
        "# print(df.sample(15))\n",
        "\n",
        "# Print only the rows where predictions are incorrect\n",
        "incorrect_predictions_Bagley = df_Bagley[df_Bagley['True Label'] != df_Bagley['Predicted Binary']]\n",
        "incorrect_predictions_MK54 = df_MK54[df_MK54['True Label'] != df_MK54['Predicted Binary']]\n",
        "incorrect_predictions_Giovanna = df_Giovanna[df_Giovanna['True Label'] != df_Giovanna['Predicted Binary']]\n",
        "incorrect_predictions_Lola = df_Lola[df_Lola['True Label'] != df_Lola['Predicted Binary']]\n",
        "\n",
        "incorrect_predictions_mean = df_mean[df_mean['True Label'] != df_mean['Predicted Binary']]\n",
        "incorrect_predictions_combined_models = df_combined_models[df_combined_models['True Label'] != df_combined_models['Predicted Binary']]\n",
        "\n",
        "\n",
        "print(\"Mistakes in prediction Bagley\")\n",
        "print(len(incorrect_predictions_Bagley))\n",
        "\n",
        "print(\"Mistakes in prediction MK54\")\n",
        "print(len(incorrect_predictions_MK54))\n",
        "\n",
        "print(\"Mistakes in prediction Giovanna\")\n",
        "print(len(incorrect_predictions_Giovanna))\n",
        "\n",
        "print(\"Mistakes in prediction Lola\")\n",
        "print(len(incorrect_predictions_Lola))\n",
        "\n",
        "print(\"Mistakes in mean prediction\")\n",
        "print(len(incorrect_predictions_mean))\n",
        "\n",
        "print(\"Mistakes in fra prediction\")\n",
        "print(len(incorrect_predictions_combined_models))\n",
        "\n",
        "print(\"\\nIncorrect Predictions Bagley\")\n",
        "print(incorrect_predictions_Bagley)\n",
        "\n",
        "print(\"\\nIncorrect Predictions MK54\")\n",
        "print(incorrect_predictions_MK54)\n",
        "\n",
        "print(\"\\nIncorrect Predictions Giovanna\")\n",
        "print(incorrect_predictions_Giovanna)\n",
        "\n",
        "print(\"\\nIncorrect Predictions Lola:\")\n",
        "print(incorrect_predictions_Lola)\n",
        "\n",
        "print(\"\\nIncorrect Predictions mean:\")\n",
        "print(incorrect_predictions_mean)\n",
        "\n",
        "print(\"\\nIncorrect Predictions combined_models:\")\n",
        "print(incorrect_predictions_combined_models)"
      ],
      "metadata": {
        "id": "HxIDQiE-EAKt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}